{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCWTvWZIXDM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zLzmYz3XT2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_size = 3\n",
        "embed_size = 2\n",
        "xmax = 2\n",
        "alpha = 0.75\n",
        "batch_size = 20\n",
        "l_rate = 0.001\n",
        "num_epochs = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl_LgvQlXWA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\n",
        "    \"In literary theory, a text is any object that can be read, whether this object is a work of literature, a street sign, an arrangement of buildings on a city block, or styles of clothing. It is a coherent set of signs that transmits some kind of informative message.[1] This set of signs is considered in terms of the informative message's content, rather than in terms of its physical form or the medium in which it is represented.\"\n",
        "    \"Within the field of literary criticism, text also refers to the original information content of a particular piece of writing; that is, the text of a work is that primal symbolic arrangement of letters as originally composed, apart from later alterations, deterioration, commentary, translations, paratext, etc. Therefore, when literary criticism is concerned with the determination of a text, it is concerned with the distinguishing of the original information content from whatever has been added to or subtracted from that content as it appears in a given textual document.\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6gtfZVNXW72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = []\n",
        "for i in range(len(corpus)):\n",
        "    sents = corpus[i].split(\".\")\n",
        "    for j in range(len(sents)):\n",
        "        tokens = tokens +sents[j].lower().split(\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxn-efi5XZty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create word to index mapping\n",
        "my_dict = list(set(tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnLQsN2CXcHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx={}\n",
        "idx2word={}\n",
        "for ind,token in enumerate(my_dict):\n",
        "    word2idx[token]=ind\n",
        "    idx2word[ind]=token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKVpCD02Xdas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ss=len(my_dict)\n",
        "zero_matrix=np.zeros((ss,ss))\n",
        "for idx in range (len(tokens)-1):\n",
        "    ind1 = word2idx[tokens[idx]]\n",
        "    ind2 = word2idx[tokens[idx+1]]\n",
        "    #print(tokens[idx],tokens[idx+1])\n",
        "    zero_matrix[ind1,ind2]+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoGBP076Xfa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coocs = np.transpose(np.nonzero(zero_matrix))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkPrN5MkXiRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weight function\n",
        "def wf(x):\n",
        "    if x < xmax:\n",
        "        return (x/xmax)**alpha\n",
        "    return 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgqcQmd1Xkaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(my_dict)\n",
        "w_list_size=len(tokens)\n",
        "\n",
        "# Set up word vectors and biases\n",
        "l_embed, r_embed = [\n",
        "    [Variable(torch.from_numpy(np.random.normal(0, 0.01, (embed_size, 1))),\n",
        "        requires_grad = True) for j in range(vocab_size)] for i in range(2)]\n",
        "l_biases, r_biases = [\n",
        "    [Variable(torch.from_numpy(np.random.normal(0, 0.01, 1)), \n",
        "        requires_grad = True) for j in range(vocab_size)] for i in range(2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68fDyz3VXmuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up optimizer\n",
        "optimizer = optim.Adam(l_embed + r_embed + l_biases + r_biases, lr = l_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5slEcZ0VXo23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch sampling function\n",
        "def gen_batch():\n",
        "    sample = np.random.choice(np.arange(len(coocs)), size=batch_size, replace=False)\n",
        "    l_vecs, r_vecs, covals, l_v_bias, r_v_bias = [], [], [], [], []\n",
        "    for chosen in sample:\n",
        "        ind = tuple(coocs[chosen])\n",
        "        l_vecs.append(l_embed[ind[0]])\n",
        "        r_vecs.append(r_embed[ind[1]])\n",
        "        covals.append(zero_matrix[ind])\n",
        "        l_v_bias.append(l_biases[ind[0]])\n",
        "        r_v_bias.append(r_biases[ind[1]])\n",
        "    return l_vecs, r_vecs, covals, l_v_bias, r_v_bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99Et-gfEXrmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bddd1ac7-2e1a-4dd8-cad3-2aa810f52daa"
      },
      "source": [
        "# Train model\n",
        "for epoch in range(num_epochs):\n",
        "    num_batches = int(w_list_size/batch_size)\n",
        "    avg_loss = 0.0\n",
        "    for batch in range(num_batches):\n",
        "        optimizer.zero_grad()\n",
        "        l_vecs, r_vecs, covals, l_v_bias, r_v_bias = gen_batch()\n",
        "        loss = sum([torch.mul((torch.dot(l_vecs[i].view(-1), r_vecs[i].view(-1)) +\n",
        "                l_v_bias[i] + r_v_bias[i] - np.log(covals[i]))**2,\n",
        "                wf(covals[i])) for i in range(batch_size)])\n",
        "        avg_loss += loss.data[0]/num_batches\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"Average loss for epoch \"+str(epoch+1)+\": \", avg_loss)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss for epoch 1:  tensor(1.4222, dtype=torch.float64)\n",
            "Average loss for epoch 2:  tensor(1.0693, dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YOI4sowXuly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize embeddings\n",
        "if embed_size == 2:\n",
        "    # Pick some random words\n",
        "    word_inds = np.random.choice(np.arange(len(my_dict)), size=10, replace=False)\n",
        "    for word_ind in word_inds:\n",
        "        # Create embedding by summing left and right embeddings\n",
        "        w_embed = (l_embed[word_ind].data + r_embed[word_ind].data).numpy()\n",
        "        x, y = w_embed[0][0], w_embed[1][0]\n",
        "        plt.scatter(x, y)\n",
        "        plt.annotate(my_dict[word_ind], xy=(x, y), xytext=(5, 2),\n",
        "            textcoords='offset points', ha='right', va='bottom')\n",
        "    plt.savefig(\"glove.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}